
import numpy as np
from chainer import Variable
from chainer import Link, Chain
import chainer.functions as F
import chainer.links as L
import chainer


class Block(chainer.Chain):

    """A convolution, batch norm, ReLU block.

    A block in a feedforward network that performs a
    convolution followed by batch normalization followed
    by a ReLU activation.

    For the convolution operation, a square filter size is used.

    Args:
        out_channels (int): The number of output channels.
        ksize (int): The size of the filter is ksize x ksize.
        pad (int): The padding to use for the convolution.

    """

    def __init__(self, out_channels, ksize, pad=1):
        super(Block, self).__init__()
        with self.init_scope():
            self.conv = L.Convolution2D(None, out_channels, ksize, pad=pad,
                                        nobias=True)
            self.bn = L.BatchNormalization(out_channels)

    def __call__(self, x):
        h = self.conv(x)
        h = self.bn(h)
        return F.relu(h)


class LeNetBase(chainer.Chain):

    def __init__(self, out_channels=10, **params):
        super(LeNetBase, self).__init__()
        with self.init_scope():
            self.block1 = Block(32, 5)
            self.block2 = Block(64, 5)
            self.block3 = Block(128, 5)
            self.fc4 = L.Linear(None, 32)
            self.fc5 = L.Linear(32, out_channels)

    def __call__(self, x):
        h = self.block1(x)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = F.reshape(h, (h.shape[0], h.shape[1] * h.shape[2] * h.shape[3]))
        h = F.relu(self.fc4(h))
        y = F.softmax(self.fc5(h))
        return y


class LeNet(chainer.Chain):

    def __init__(self, out_channels=10, style=None, **params):
        super(LeNet, self).__init__()
        with self.init_scope():
            self.block1 = Block(32, 5)
            self.block2 = Block(64, 5)
            self.block3 = Block(128, 5)
            self.fc4 = L.Linear(1024, 32)
            self.fc5 = L.Linear(32, out_channels)
            self.style = style

    def __call__(self, x):
        h = self.block1(x)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        g = self.style.feature(x)
        # print('g', g.shape, h.shape)

        h = F.concat([h, g])
        h = F.reshape(h, (h.shape[0], h.shape[1] * h.shape[2] * h.shape[3]))
        h = F.relu(self.fc4(h))
        y = F.softmax(self.fc5(h))
        return y

    def call(self, x, g=None):
        h = self.block1(x)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        # h = F.flatten(h)
        if g is None:
            g = self.style.feature(x)

        h = F.concat([h, g])

        h = F.reshape(h, (h.shape[0], h.shape[1] * h.shape[2] * h.shape[3]))
        h = F.relu(self.fc4(h))
        y = F.softmax(self.fc5(h))
        return y



class StyleNet(chainer.Chain):

    def __init__(self, out_channels=10):
        super(StyleNet, self).__init__()
        with self.init_scope():
            self.block1 = Block(32, 5)
            self.block2 = Block(64, 5)
            self.block3 = Block(128, 5)
            self.fc4 = L.Linear(None, 32)
            self.fc5 = L.Linear(32, out_channels)

    def feature(self, x):
        h = self.block1(x)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block3(h)
        g = F.max_pooling_2d(h, ksize=2, stride=2)
        return g

    def __call__(self, x):
        h = self.block1(x)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)
        h = self.block3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.relu(self.fc4(h))
        y = F.softmax(self.fc5(h))
        return y


class FC(chainer.Chain):
    def __init__(self, out_channels):
        super(FC, self).__init__()
        with self.init_scope():
            self.fc1 = L.Linear(None, out_channels)

    def __call__(self, x):
        y = F.softmax(self.fc1(x))
        return y


class FC2(chainer.Chain):
    def __init__(self, out_channels):
        super(FC2, self).__init__()
        with self.init_scope():
            self.fc1 = L.Linear(None, 32)
            self.fc2 = L.Linear(32, out_channels)

    def __call__(self, x):
        h = F.relu(self.fc1(x))
        y = F.softmax(self.fc2(h))
        return y


import chainer
import chainer.functions as F
import chainer.links as L


class Block(chainer.Chain):

    """A convolution, batch norm, ReLU block.

    A block in a feedforward network that performs a
    convolution followed by batch normalization followed
    by a ReLU activation.

    For the convolution operation, a square filter size is used.

    Args:
        out_channels (int): The number of output channels.
        ksize (int): The size of the filter is ksize x ksize.
        pad (int): The padding to use for the convolution.

    """

    def __init__(self, out_channels, ksize, pad=1):
        super(Block, self).__init__()
        with self.init_scope():
            self.conv = L.Convolution2D(None, out_channels, ksize, pad=pad,
                                        nobias=True)
            self.bn = L.BatchNormalization(out_channels)

    def __call__(self, x):
        h = self.conv(x)
        h = self.bn(h)
        return F.relu(h)


class VGG(chainer.Chain):

    """A VGG-style network for very small images.

    This model is based on the VGG-style model from
    http://torch.ch/blog/2015/07/30/cifar.html
    which is based on the network architecture from the paper:
    https://arxiv.org/pdf/1409.1556v6.pdf

    This model is intended to be used with either RGB or greyscale input
    images that are of size 32x32 pixels, such as those in the CIFAR10
    and CIFAR100 datasets.

    On CIFAR10, it achieves approximately 89% accuracy on the test set with
    no data augmentation.

    On CIFAR100, it achieves approximately 63% accuracy on the test set with
    no data augmentation.

    Args:
        class_labels (int): The number of class labels.

    """

    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

    def __call__(self, x, feature=False):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        if feature:
            return h

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)


class VGGFeature(chainer.Chain):

    """A VGG-style network for very small images.

    This model is based on the VGG-style model from
    http://torch.ch/blog/2015/07/30/cifar.html
    which is based on the network architecture from the paper:
    https://arxiv.org/pdf/1409.1556v6.pdf

    This model is intended to be used with either RGB or greyscale input
    images that are of size 32x32 pixels, such as those in the CIFAR10
    and CIFAR100 datasets.

    On CIFAR10, it achieves approximately 89% accuracy on the test set with
    no data augmentation.

    On CIFAR100, it achieves approximately 63% accuracy on the test set with
    no data augmentation.

    Args:
        class_labels (int): The number of class labels.

    """

    def __init__(self, pretrain=True):
        super(VGGFeature, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)

        if pretrain:
            print('loading VGG16Layers...')
            from chainer.links import VGG16Layers
            vgg = VGG16Layers()
            # self.block1_1.conv = vgg.conv1_1
            self.block1_2.conv = vgg.conv1_2
            self.block2_1.conv = vgg.conv2_1
            self.block2_2.conv = vgg.conv2_2
            self.block3_1.conv = vgg.conv3_1
            self.block3_2.conv = vgg.conv3_2
            self.block3_3.conv = vgg.conv3_3
            self.block4_1.conv = vgg.conv4_1
            self.block4_2.conv = vgg.conv4_2
            self.block4_3.conv = vgg.conv4_3
            self.block5_1.conv = vgg.conv5_1
            self.block5_2.conv = vgg.conv5_2
            self.block5_3.conv = vgg.conv5_3

    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        return h


class VGGFeatureLight(chainer.Chain):

    """A VGG-style network for very small images.

    This model is based on the VGG-style model from
    http://torch.ch/blog/2015/07/30/cifar.html
    which is based on the network architecture from the paper:
    https://arxiv.org/pdf/1409.1556v6.pdf

    This model is intended to be used with either RGB or greyscale input
    images that are of size 32x32 pixels, such as those in the CIFAR10
    and CIFAR100 datasets.

    On CIFAR10, it achieves approximately 89% accuracy on the test set with
    no data augmentation.

    On CIFAR100, it achieves approximately 63% accuracy on the test set with
    no data augmentation.

    Args:
        class_labels (int): The number of class labels.

    """

    def __init__(self, **params):
        super(VGGFeatureLight, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)

    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        return h
